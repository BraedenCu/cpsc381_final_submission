{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bec30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c45b9e3",
   "metadata": {},
   "source": [
    "# Note: Our dataset is custom.\n",
    "\n",
    "You MUST ensure that you have properly downloaded the dataset and placed it in your google drive at the base level. You cannot simply read the dataset as a \"shared folder\" rather, you must first download it locally then reupload so that it is stored in your google drive. Please be concious of this, otherwise erranous output might be misinterpreted as an issue with our code when in reality is an issue with how the dataset has been set up on your google drive. You can also try adding a shortcut to the dataset to your personal drive which may work but it is only garunteed if the full dataset is stored in your drive  as those are the conditions under which we trained our model.\n",
    "\n",
    "## Subnote\n",
    "\n",
    "If you get a \"values unpack\" error (something along those lines) it is due to incorrect setup of our dataset in your google drive. It is not due to an error in our code. To correct this error, ensure that the steps in the above paragraph were followed to a tee. Please follow up with us if you have questions.\n",
    "\n",
    "\n",
    "# Note Two: Training takes a longgggg time.\n",
    "\n",
    "You might notice that we only ran our model for two epochs. This is because training is very very slow, even on A100 GPU's. We are broke college students and cannot continue to purchase colab compute units, so we cut things off once we saw performance that was good enough to meet our project proposal requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66d28f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL RUN!!!!!\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from collections import deque, Counter\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import (\n",
    "    GlobalAveragePooling2D, Dense, Input,\n",
    "    TimeDistributed, LSTM, Dropout\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, LambdaCallback,\n",
    "    ReduceLROnPlateau\n",
    ")\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 3. Params\n",
    "base_path        = \"/content/drive/MyDrive/drone_datasets/final_data\"\n",
    "clip_length      = 16\n",
    "clip_stride      = 4      # 75% overlap\n",
    "batch_size       = 8\n",
    "file_seed        = 42\n",
    "clip_shuffle     = 500\n",
    "learning_rate    = 1e-3\n",
    "max_epochs       = 2      # you can still control # of epochs, we set to 2 for now to not waste all of our compute, this is a very compute intensive model!!\n",
    "dropout_rate     = 0.5\n",
    "recurrent_do     = 0.1\n",
    "lstm_units       = 64\n",
    "subset_fraction  = 0.1    # fraction of data to use per epoch, 0.1 to minimze compute waste\n",
    "\n",
    "# 4. Gather all files & labels\n",
    "def list_videos(subdir):\n",
    "    exts = ('avi','mp4','mov')\n",
    "    files = []\n",
    "    folder = os.path.join(base_path, subdir)\n",
    "    for e in exts:\n",
    "        files += glob.glob(os.path.join(folder, '**', f'*.{e}'),\n",
    "                           recursive=True)\n",
    "    return files\n",
    "\n",
    "wave_files     = list_videos(\"wave\")\n",
    "not_wave_files = list_videos(\"not_wave\")\n",
    "all_files      = [(p,1) for p in wave_files] + [(p,0) for p in not_wave_files]\n",
    "random.seed(file_seed)\n",
    "random.shuffle(all_files)\n",
    "\n",
    "# 5. Split file-level 73/15/15\n",
    "n        = len(all_files)\n",
    "n_train  = int(0.70 * n)\n",
    "n_val    = int(0.15 * n)\n",
    "train_files = all_files[:n_train]\n",
    "val_files   = all_files[n_train:n_train+n_val]\n",
    "test_files  = all_files[n_train+n_val:]\n",
    "\n",
    "print(\"Raw train distribution:\", Counter(l for _,l in train_files))\n",
    "\n",
    "# 6. Oversample positives (if needed)\n",
    "pos = [f for f in train_files if f[1]==1]\n",
    "neg = [f for f in train_files if f[1]==0]\n",
    "if pos:\n",
    "    repeat_factor = len(neg) // len(pos)\n",
    "    train_files = neg + pos * repeat_factor\n",
    "    random.shuffle(train_files)\n",
    "    print(\"Balanced train distribution:\", Counter(l for _,l in train_files))\n",
    "else:\n",
    "    print(\"Warning: no positive samples in training set!\")\n",
    "\n",
    "# 7. Compute total_train_clips for steps_per_epoch (and subset)\n",
    "#total_train_clips = 0\n",
    "#for path, _ in train_files:\n",
    "#    cap = cv2.VideoCapture(path)\n",
    "#    frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#    cap.release()\n",
    "#    clips = max(0, (frames - clip_length)//clip_stride + 1)\n",
    "#    total_train_clips += clips\n",
    "\n",
    "#full_steps_per_epoch = max(1, total_train_clips // batch_size)\n",
    "#steps_per_epoch = max(1, int(full_steps_per_epoch * subset_fraction))\n",
    "#print(f\"Total train clips: {total_train_clips}, full steps/epoch: {full_steps_per_epoch},\")\n",
    "#print(f\"→ using {steps_per_epoch} steps ({subset_fraction*100:.1f}% of data) per epoch\")\n",
    "\n",
    "# 7. Compute total_train_clips for steps_per_epoch (and subset), hasty route (for optimized speed)\n",
    "# we only open 100 files, not all of them. its good enough since we just need an approximate schedule.\n",
    "# if we want to be incredibly precise and use the entire dataset, uncomment the above code.\n",
    "sample_size = min(100, len(train_files))\n",
    "sampled     = random.sample(train_files, sample_size)\n",
    "\n",
    "clip_counts = []\n",
    "for path, _ in sampled:\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    cap.release()\n",
    "    clip_counts.append(max(0, (frames - clip_length)//clip_stride + 1))\n",
    "\n",
    "avg_clips = sum(clip_counts) / len(clip_counts)\n",
    "total_train_clips = int(avg_clips * len(train_files))\n",
    "\n",
    "full_steps_per_epoch = max(1, total_train_clips // batch_size)\n",
    "steps_per_epoch      = max(1, int(full_steps_per_epoch * subset_fraction))\n",
    "print(f\"Estimated total clips: {total_train_clips}\")\n",
    "print(f\"→ using {steps_per_epoch} steps ({subset_fraction*100:.1f}% of data) per epoch\")\n",
    "\n",
    "# end of custom optimized step 7.\n",
    "\n",
    "# 8. Clip-extraction helper\n",
    "def extract_clips(path):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    buf, clips, idx = deque(maxlen=clip_length), [], 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = cv2.resize(frame, (224,224))\n",
    "        buf.append(frame)\n",
    "        idx += 1\n",
    "        if idx >= clip_length and (idx - clip_length) % clip_stride == 0:\n",
    "            clips.append(np.array(buf, dtype=np.float32))\n",
    "    cap.release()\n",
    "    if clips:\n",
    "        return preprocess_input(np.stack(clips, axis=0))\n",
    "    else:\n",
    "        return np.zeros((0,clip_length,224,224,3), dtype=np.float32)\n",
    "\n",
    "# 9. TF loader + unbatch\n",
    "def load_and_split(path, label):\n",
    "    def _py(p, l):\n",
    "        p_str = p.numpy().decode('utf-8')\n",
    "        clips_np = extract_clips(p_str)\n",
    "        labels_np = np.full((clips_np.shape[0],), l.numpy(), np.int32)\n",
    "        return clips_np, labels_np\n",
    "\n",
    "    clips, lbls = tf.py_function(\n",
    "        func=_py,\n",
    "        inp=[path, label],\n",
    "        Tout=[tf.float32, tf.int32]\n",
    "    )\n",
    "    clips.set_shape([None, clip_length,224,224,3])\n",
    "    lbls.set_shape([None])\n",
    "    return clips, lbls\n",
    "\n",
    "def make_clip_dataset(files):\n",
    "    paths, labels = zip(*files)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((list(paths), list(labels)))\n",
    "    return ds.map(load_and_split, num_parallel_calls=tf.data.AUTOTUNE).unbatch()\n",
    "\n",
    "train_ds = (\n",
    "    make_clip_dataset(train_files)\n",
    "    .shuffle(clip_shuffle, seed=file_seed)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "    .repeat()\n",
    ")\n",
    "\n",
    "val_ds = (\n",
    "    make_clip_dataset(val_files)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "test_ds = (\n",
    "    make_clip_dataset(test_files)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# 10. Build model with updated regularization\n",
    "cnn_base = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "cnn_base.trainable = False\n",
    "\n",
    "inp = Input((clip_length,224,224,3))\n",
    "x   = TimeDistributed(cnn_base)(inp)\n",
    "x   = TimeDistributed(GlobalAveragePooling2D())(x)\n",
    "x   = Dropout(dropout_rate)(x)\n",
    "x   = LSTM(\n",
    "    lstm_units,\n",
    "    dropout=dropout_rate,\n",
    "    recurrent_dropout=recurrent_do,\n",
    "    kernel_regularizer=l2(1e-4),\n",
    "    recurrent_regularizer=l2(1e-4)\n",
    ")(x)\n",
    "out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inp, out)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy','precision','recall','auc']\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "# 11. Callbacks\n",
    "class F1Metrics(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, val_ds):\n",
    "        super().__init__()\n",
    "        self.val_ds = val_ds\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_true, y_pred = [], []\n",
    "        for Xb, yb in self.val_ds:\n",
    "            probs = self.model.predict(Xb, verbose=0).flatten()\n",
    "            preds = (probs > 0.5).astype(int)\n",
    "            y_true.extend(yb.numpy().tolist())\n",
    "            y_pred.extend(preds.tolist())\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        print(f\" — val_f1: {f1:.4f}\")\n",
    "        logs['val_f1'] = f1\n",
    "\n",
    "print_cb   = LambdaCallback(on_epoch_end=lambda e, logs:\n",
    "    print(f\"\\nEpoch {e+1}: loss={logs['loss']:.4f}, val_auc={logs['val_auc']:.4f}\", end='')\n",
    ")\n",
    "f1_cb      = F1Metrics(val_ds)\n",
    "lr_cb      = ReduceLROnPlateau(monitor='val_f1', factor=0.5, patience=3, mode='max', verbose=1)\n",
    "early_stop = EarlyStopping(monitor='val_f1', mode='max', patience=3, restore_best_weights=True)\n",
    "\n",
    "class_weight = {\n",
    "  0: 1.0,\n",
    "  1: len(neg) / len(pos)   # adjust as before\n",
    "}\n",
    "\n",
    "# 12. Train\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=max_epochs,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    class_weight=class_weight,\n",
    "    callbacks=[print_cb, f1_cb, lr_cb, early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 13. Plot Loss & Validation F1\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history.get('val_loss', []), label='val_loss')\n",
    "plt.title('Loss'); plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history.get('val_f1', []), label='val_f1')\n",
    "plt.title('Validation F1'); plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 14. Evaluate on test set\n",
    "print(\"\\nBatch-level evaluation:\")\n",
    "model.evaluate(test_ds, verbose=1)\n",
    "\n",
    "# 15. Per-video accuracy\n",
    "video_results = []\n",
    "for path, label in test_files:\n",
    "    clips = extract_clips(path)\n",
    "    if clips.shape[0] == 0: continue\n",
    "    preds = model.predict(clips, batch_size=batch_size, verbose=0).flatten()\n",
    "    avg_p = preds.mean()\n",
    "    pred_label = int(avg_p > 0.5)\n",
    "    video_results.append((path, label, avg_p, pred_label))\n",
    "\n",
    "n_vid = len(video_results)\n",
    "n_corr = sum(1 for _,lbl,_,pred in video_results if pred==lbl)\n",
    "print(f\"\\nPer-video accuracy on {n_vid} videos: {n_corr/n_vid:.4f}\")\n",
    "\n",
    "# 16. Save final model\n",
    "save_path = os.path.join(base_path, 'wave_sequence_model_final5.keras')\n",
    "model.save(save_path)\n",
    "print(f\"Saved model to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
