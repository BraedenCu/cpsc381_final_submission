{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66d28f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL RUN!!!!!\n",
    "\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from collections import deque, Counter\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import (\n",
    "    GlobalAveragePooling2D, Dense, Input,\n",
    "    TimeDistributed, LSTM, Dropout\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, LambdaCallback,\n",
    "    ReduceLROnPlateau\n",
    ")\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 3. Params\n",
    "base_path        = \"/content/drive/MyDrive/drone_datasets/final_data\"\n",
    "clip_length      = 16\n",
    "clip_stride      = 4      # 75% overlap\n",
    "batch_size       = 8\n",
    "file_seed        = 42\n",
    "clip_shuffle     = 500\n",
    "learning_rate    = 1e-3\n",
    "max_epochs       = 2     # you can still control # of epochs\n",
    "dropout_rate     = 0.5\n",
    "recurrent_do     = 0.1\n",
    "lstm_units       = 64\n",
    "subset_fraction  = 0.1    # fraction of data to use per epoch\n",
    "\n",
    "# 4. Gather all files & labels\n",
    "def list_videos(subdir):\n",
    "    exts = ('avi','mp4','mov')\n",
    "    files = []\n",
    "    folder = os.path.join(base_path, subdir)\n",
    "    for e in exts:\n",
    "        files += glob.glob(os.path.join(folder, '**', f'*.{e}'),\n",
    "                           recursive=True)\n",
    "    return files\n",
    "\n",
    "wave_files     = list_videos(\"wave\")\n",
    "not_wave_files = list_videos(\"not_wave\")\n",
    "all_files      = [(p,1) for p in wave_files] + [(p,0) for p in not_wave_files]\n",
    "random.seed(file_seed)\n",
    "random.shuffle(all_files)\n",
    "\n",
    "# 5. Split file-level 73/15/15\n",
    "n        = len(all_files)\n",
    "n_train  = int(0.70 * n)\n",
    "n_val    = int(0.15 * n)\n",
    "train_files = all_files[:n_train]\n",
    "val_files   = all_files[n_train:n_train+n_val]\n",
    "test_files  = all_files[n_train+n_val:]\n",
    "\n",
    "print(\"Raw train distribution:\", Counter(l for _,l in train_files))\n",
    "\n",
    "# 6. Oversample positives (if needed)\n",
    "pos = [f for f in train_files if f[1]==1]\n",
    "neg = [f for f in train_files if f[1]==0]\n",
    "if pos:\n",
    "    repeat_factor = len(neg) // len(pos)\n",
    "    train_files = neg + pos * repeat_factor\n",
    "    random.shuffle(train_files)\n",
    "    print(\"Balanced train distribution:\", Counter(l for _,l in train_files))\n",
    "else:\n",
    "    print(\"Warning: no positive samples in training set!\")\n",
    "\n",
    "# 7. Compute total_train_clips for steps_per_epoch (and subset)\n",
    "total_train_clips = 0\n",
    "for path, _ in train_files:\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    cap.release()\n",
    "    clips = max(0, (frames - clip_length)//clip_stride + 1)\n",
    "    total_train_clips += clips\n",
    "\n",
    "full_steps_per_epoch = max(1, total_train_clips // batch_size)\n",
    "steps_per_epoch = max(1, int(full_steps_per_epoch * subset_fraction))\n",
    "print(f\"Total train clips: {total_train_clips}, full steps/epoch: {full_steps_per_epoch},\")\n",
    "print(f\"→ using {steps_per_epoch} steps ({subset_fraction*100:.1f}% of data) per epoch\")\n",
    "\n",
    "# 8. Clip-extraction helper\n",
    "def extract_clips(path):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    buf, clips, idx = deque(maxlen=clip_length), [], 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = cv2.resize(frame, (224,224))\n",
    "        buf.append(frame)\n",
    "        idx += 1\n",
    "        if idx >= clip_length and (idx - clip_length) % clip_stride == 0:\n",
    "            clips.append(np.array(buf, dtype=np.float32))\n",
    "    cap.release()\n",
    "    if clips:\n",
    "        return preprocess_input(np.stack(clips, axis=0))\n",
    "    else:\n",
    "        return np.zeros((0,clip_length,224,224,3), dtype=np.float32)\n",
    "\n",
    "# 9. TF loader + unbatch\n",
    "def load_and_split(path, label):\n",
    "    def _py(p, l):\n",
    "        p_str = p.numpy().decode('utf-8')\n",
    "        clips_np = extract_clips(p_str)\n",
    "        labels_np = np.full((clips_np.shape[0],), l.numpy(), np.int32)\n",
    "        return clips_np, labels_np\n",
    "\n",
    "    clips, lbls = tf.py_function(\n",
    "        func=_py,\n",
    "        inp=[path, label],\n",
    "        Tout=[tf.float32, tf.int32]\n",
    "    )\n",
    "    clips.set_shape([None, clip_length,224,224,3])\n",
    "    lbls.set_shape([None])\n",
    "    return clips, lbls\n",
    "\n",
    "def make_clip_dataset(files):\n",
    "    paths, labels = zip(*files)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((list(paths), list(labels)))\n",
    "    return ds.map(load_and_split, num_parallel_calls=tf.data.AUTOTUNE).unbatch()\n",
    "\n",
    "train_ds = (\n",
    "    make_clip_dataset(train_files)\n",
    "    .shuffle(clip_shuffle, seed=file_seed)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "    .repeat()\n",
    ")\n",
    "\n",
    "val_ds = (\n",
    "    make_clip_dataset(val_files)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "test_ds = (\n",
    "    make_clip_dataset(test_files)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# 10. Build model with updated regularization\n",
    "cnn_base = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "cnn_base.trainable = False\n",
    "\n",
    "inp = Input((clip_length,224,224,3))\n",
    "x   = TimeDistributed(cnn_base)(inp)\n",
    "x   = TimeDistributed(GlobalAveragePooling2D())(x)\n",
    "x   = Dropout(dropout_rate)(x)\n",
    "x   = LSTM(\n",
    "    lstm_units,\n",
    "    dropout=dropout_rate,\n",
    "    recurrent_dropout=recurrent_do,\n",
    "    kernel_regularizer=l2(1e-4),\n",
    "    recurrent_regularizer=l2(1e-4)\n",
    ")(x)\n",
    "out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inp, out)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy','precision','recall','auc']\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "# 11. Callbacks\n",
    "class F1Metrics(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, val_ds):\n",
    "        super().__init__()\n",
    "        self.val_ds = val_ds\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_true, y_pred = [], []\n",
    "        for Xb, yb in self.val_ds:\n",
    "            probs = self.model.predict(Xb, verbose=0).flatten()\n",
    "            preds = (probs > 0.5).astype(int)\n",
    "            y_true.extend(yb.numpy().tolist())\n",
    "            y_pred.extend(preds.tolist())\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        print(f\" — val_f1: {f1:.4f}\")\n",
    "        logs['val_f1'] = f1\n",
    "\n",
    "print_cb   = LambdaCallback(on_epoch_end=lambda e, logs:\n",
    "    print(f\"\\nEpoch {e+1}: loss={logs['loss']:.4f}, val_auc={logs['val_auc']:.4f}\", end='')\n",
    ")\n",
    "f1_cb      = F1Metrics(val_ds)\n",
    "lr_cb      = ReduceLROnPlateau(monitor='val_f1', factor=0.5, patience=3, mode='max', verbose=1)\n",
    "early_stop = EarlyStopping(monitor='val_f1', mode='max', patience=3, restore_best_weights=True)\n",
    "\n",
    "class_weight = {\n",
    "  0: 1.0,\n",
    "  1: len(neg) / len(pos)   # adjust as before\n",
    "}\n",
    "\n",
    "# 12. Train\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=max_epochs,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    class_weight=class_weight,\n",
    "    callbacks=[print_cb, f1_cb, lr_cb, early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 13. Plot Loss & Validation F1\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history.get('val_loss', []), label='val_loss')\n",
    "plt.title('Loss'); plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history.get('val_f1', []), label='val_f1')\n",
    "plt.title('Validation F1'); plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 14. Evaluate on test set\n",
    "print(\"\\nBatch-level evaluation:\")\n",
    "model.evaluate(test_ds, verbose=1)\n",
    "\n",
    "# 15. Per-video accuracy\n",
    "video_results = []\n",
    "for path, label in test_files:\n",
    "    clips = extract_clips(path)\n",
    "    if clips.shape[0] == 0: continue\n",
    "    preds = model.predict(clips, batch_size=batch_size, verbose=0).flatten()\n",
    "    avg_p = preds.mean()\n",
    "    pred_label = int(avg_p > 0.5)\n",
    "    video_results.append((path, label, avg_p, pred_label))\n",
    "\n",
    "n_vid = len(video_results)\n",
    "n_corr = sum(1 for _,lbl,_,pred in video_results if pred==lbl)\n",
    "print(f\"\\nPer-video accuracy on {n_vid} videos: {n_corr/n_vid:.4f}\")\n",
    "\n",
    "# 16. Save final model\n",
    "save_path = os.path.join(base_path, 'wave_sequence_model_final5.keras')\n",
    "model.save(save_path)\n",
    "print(f\"Saved model to {save_path}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
